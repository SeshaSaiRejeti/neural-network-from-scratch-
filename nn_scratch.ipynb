{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba7bf4b1-ac39-49d6-8f92-97bb1d7d401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f35061d-aaaf-49c3-a9b1-ce002c728176",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = make_moons(n_samples = 400, noise = 0.2, random_state = 42)\n",
    "\n",
    "x = x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a28cb34f-78ac-4688-aedf-274780511b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.reshape(1,y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7158677-9ab6-41f1-9a90-2cc48597801f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 400)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b58b0e2-f624-4bdd-806e-106644a705d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape\n",
    "m = x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "930881e4-5452-4ab7-a750-c1fc3f616164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the 0th iteration the cost is0.6931471563941191\n",
      "for the 1000th iteration the cost is0.6931471073424039\n",
      "for the 2000th iteration the cost is0.6931470913152694\n",
      "for the 3000th iteration the cost is0.6931470712214758\n",
      "for the 4000th iteration the cost is0.6931470440217783\n",
      "for the 5000th iteration the cost is0.6931470057304482\n",
      "for the 6000th iteration the cost is0.6931469494101791\n",
      "for the 7000th iteration the cost is0.6931468612860857\n",
      "for the 8000th iteration the cost is0.6931467134701343\n",
      "for the 9000th iteration the cost is0.6931464392992086\n",
      "for the 10000th iteration the cost is0.6931458462317022\n",
      "for the 11000th iteration the cost is0.6931441691945\n",
      "for the 12000th iteration the cost is0.6931354538198636\n",
      "for the 13000th iteration the cost is0.6886827987874665\n",
      "for the 14000th iteration the cost is0.08157084269633484\n",
      "for the 15000th iteration the cost is0.07705474038184473\n",
      "for the 16000th iteration the cost is0.07353210373973747\n",
      "for the 17000th iteration the cost is0.06629913386643635\n",
      "for the 18000th iteration the cost is0.06491832921447656\n",
      "for the 19000th iteration the cost is0.06206264243578897\n",
      "for the 20000th iteration the cost is0.060205881965409616\n",
      "for the 21000th iteration the cost is0.056033707234825536\n",
      "for the 22000th iteration the cost is0.05180960564483145\n",
      "for the 23000th iteration the cost is0.05275819261899194\n",
      "for the 24000th iteration the cost is0.05410741103891426\n",
      "for the 25000th iteration the cost is0.05233741251400221\n",
      "for the 26000th iteration the cost is0.0517432107871675\n",
      "for the 27000th iteration the cost is0.04925610398971802\n",
      "for the 28000th iteration the cost is0.049755258793898784\n",
      "for the 29000th iteration the cost is0.04683449674373197\n",
      "for the 30000th iteration the cost is0.04664644365220258\n",
      "for the 31000th iteration the cost is0.04567721095703403\n",
      "for the 32000th iteration the cost is0.04428176441162794\n",
      "for the 33000th iteration the cost is0.045459801156305255\n",
      "for the 34000th iteration the cost is0.03775331777512155\n",
      "for the 35000th iteration the cost is0.03590308513456068\n",
      "for the 36000th iteration the cost is0.040646179764096436\n",
      "for the 37000th iteration the cost is0.032707528750458736\n",
      "for the 38000th iteration the cost is0.03817633411233776\n",
      "for the 39000th iteration the cost is0.043232957243483784\n",
      "for the 40000th iteration the cost is0.03891798594188734\n",
      "for the 41000th iteration the cost is0.0314511943288565\n",
      "for the 42000th iteration the cost is0.03266070275367541\n",
      "for the 43000th iteration the cost is0.034919333373059386\n",
      "for the 44000th iteration the cost is0.034736215894547263\n",
      "for the 45000th iteration the cost is0.0550123688592079\n",
      "for the 46000th iteration the cost is0.031147642822407055\n",
      "for the 47000th iteration the cost is0.042412645921048195\n",
      "for the 48000th iteration the cost is0.026081985905876577\n",
      "for the 49000th iteration the cost is0.024399151973131587\n",
      "for the 50000th iteration the cost is0.024935379920995503\n",
      "for the 51000th iteration the cost is0.024957352058114814\n",
      "for the 52000th iteration the cost is0.02449874988792985\n",
      "for the 53000th iteration the cost is0.02356327801663877\n",
      "for the 54000th iteration the cost is0.02206781587820417\n",
      "for the 55000th iteration the cost is0.02200793851236084\n",
      "for the 56000th iteration the cost is0.021567728673278844\n",
      "for the 57000th iteration the cost is0.021213861294333\n",
      "for the 58000th iteration the cost is0.020757159376574692\n",
      "for the 59000th iteration the cost is0.020387955853618966\n",
      "for the 60000th iteration the cost is0.020168681954656882\n",
      "for the 61000th iteration the cost is0.019600522001957638\n",
      "for the 62000th iteration the cost is0.019703531712915973\n",
      "for the 63000th iteration the cost is0.019640019736710326\n",
      "for the 64000th iteration the cost is0.01894000022262796\n",
      "for the 65000th iteration the cost is0.0203008759008453\n",
      "for the 66000th iteration the cost is0.019207849289155813\n",
      "for the 67000th iteration the cost is0.019181153925113253\n",
      "for the 68000th iteration the cost is0.01952852062640635\n",
      "for the 69000th iteration the cost is0.01746106488661476\n",
      "for the 70000th iteration the cost is0.016454698833027748\n",
      "for the 71000th iteration the cost is0.017105194240284415\n",
      "for the 72000th iteration the cost is0.01683075136003604\n",
      "Training accuracy: 0.9925\n"
     ]
    }
   ],
   "source": [
    "#defining the architecture\n",
    "architecture = [2, 20, 10, 5 , 1]\n",
    "#initialising the parasmeters\n",
    "np.random.seed(42)\n",
    "parameters = {}\n",
    "for l in range(1,len(architecture)):\n",
    "    parameters[\"w\" + str(l)] = np.random.randn(architecture[l], architecture[l-1]) * 0.01\n",
    "    parameters[\"b\" + str(l)] = np.zeros((architecture[l],1))\n",
    "    \n",
    "#defining the hyperparameters\n",
    "learning_rate = 0.3\n",
    "n_iterations = 73000\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    caches = {}\n",
    "    a = x\n",
    "    #forward pass  --->>> all hidden layers done with relu and output is done by sigmoid\n",
    "    for l in range(1,len(architecture)):\n",
    "        z = np.dot(parameters[\"w\" + str(l)], a) + parameters[\"b\" + str(l)]\n",
    "        if l != (len(architecture) - 1):\n",
    "            a = np.maximum(0,z)\n",
    "        else :\n",
    "            a = 1/(1+np.exp(-z))\n",
    "        caches[\"z\" + str(l)] = z\n",
    "        caches[\"a\" + str(l)] = a\n",
    "    #output for the iteration\n",
    "    A = a\n",
    "    #loss function that too only for a binary classification task and we will use mse for regression and the activation funtions also change for regression task and for multi class classification we use softmax for the task\n",
    "    loss = -np.mean(y*np.log(A + 1e-8)+(1-y)*np.log(1-A + 1e-8))\n",
    "    #backprop for any architecture \n",
    "    grads = {}\n",
    "    da = A - y  #although its not mathematically crct , the crct one is dz = A-y, we are doing this for implementation purpose\n",
    "    for l in reversed(range(1,len(architecture))):\n",
    "        a_prev = x if l == 1 else caches[\"a\" + str(l-1)]\n",
    "        if l == len(architecture) - 1 :\n",
    "            dz = da\n",
    "        else :\n",
    "            dz = da * (caches[\"z\" + str(l)]> 0)\n",
    "        grads[\"dw\" + str(l)] = (1/m)*np.dot(dz,a_prev.T)\n",
    "        grads[\"db\" + str(l)] = (1/m) * np.sum(dz, keepdims = True, axis = 1)\n",
    "        da = np.dot(parameters[\"w\" + str(l)].T,dz)\n",
    "\n",
    "\n",
    "    # update parameters\n",
    "    for l in range(1, len(architecture)):\n",
    "        parameters[\"w\" + str(l)] -= learning_rate*grads[\"dw\" + str(l)]\n",
    "        parameters[\"b\" + str(l)] -= learning_rate*grads[\"db\" + str(l)]\n",
    "    #compute loss for every 1000th iteration\n",
    "    if i%1000 == 0:\n",
    "        print(f\"for the {i}th iteration the cost is{loss}\")\n",
    "\n",
    "#prediction\n",
    "a = x\n",
    "for l in range(1, len(architecture)):\n",
    "    z = np.dot(parameters[\"w\" + str(l)], a) + parameters[\"b\" + str(l)]\n",
    "    if l != len(architecture) - 1:\n",
    "        a = np.maximum(0, z)\n",
    "    else:\n",
    "        a = 1 / (1 + np.exp(-z))\n",
    "predictions = a > 0.5\n",
    "accuracy = np.mean(predictions == y)\n",
    "print(f\"Training accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# I tweaked the hyperparameters manually i hadnt implemented any tuning technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d9b04f-5b23-4c52-9367-d567dfaff6de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
